# 第二章 数学基础

## 2.1 网络架构
1. 损失函数（loss function）
2. 优化器（optimizer）
3. 在训练和测试过程中需要监控的指标(metric)
#### eg:
```
newtwork.compile(optimizer='rmsprop',
                 loss='categorical_crossentropy',
                 metrics=['accuracy']
```

## 2.2 张量

#### 2.2.1 标量（0D张量）
包含一个数字的张量叫标量，在numpy中，一个float32,float64的数字是一个标量,有0个轴（ndim==0）。

#### 2.2.2 向量（1D张量）
数字组成的数组叫向量（vector）或一维张量，只有一个轴（ndim==1）,若是5D张量，即为5个轴（ndim==5）,
也可说5阶张量，阶数为轴的个数（行）。
向量的形状只有一个数，若有5个元素，为（5，），标量的形状为空，既（）。

#### 2.2.3 矩阵（2D张量）
向量组成的数组为矩阵（matrix），二维张量（2D张量）(ndim==2)，第一个轴的元素叫行，第二个轴的元素叫列。
若一个矩阵有3行5列，其形状为（3，5）。

#### 2.2.4 3D张量或更高维张量
将多个矩阵组成一个新的数组，可以得到3D张量（ndim==3），其形状如（3，3，5），以此类推

#### 2.2.5 数据批量
数据张量的第一个轴(0轴)，都是样本轴，也就是样本量，比如一个2D张量，第一个轴为**样本轴**，第二个轴
为**特征轴**

## 2.3 神经网络的’引擎‘，基于梯度优化

神经网络架构中层的权重，也叫做层的属性，或者叫做可训练参数。这些参数初始值可以使用随机初始化。 训练循环（training loop）：

1. 抽取训练样本x和对应目标y组成的数据批量
2. 在x上运行网络，这一步叫做前向传播（forward pass），得到预测值y_pred
3. 计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离
4. 更新网络的所有权重，使网络在这批数据上的损失略微下降

#### 2.3.1 导数
导数：连续的光滑函数可以对某个点求导，导数就是该点的斜率。

#### 2.3.2 梯度
梯度：张量运算的导数，是导数在多元函数上的推广，使用张量作为多元函数的输入，梯度就是该点的曲率（Curvature）

#### 随机梯度下降
小批量随机梯度下降（Mini-batch Stochastic Gradient Descent，小批量SGD）：

1. 抽取训练样本和对应目标组成的数据批量
2. 在训练样本上训练网络，得到预测值
3. 计算网格在这批数据上的损失，用于衡量预测值和对应目标之间的距离
4. 计算损失相对于网络参数的梯度（一次反向传播）
5. 将参数沿着梯度的反方向移动一点，从而使这批数据上的损失减小一点
随机（stochastic）是指每批数据都是随机抽取的

SGD的多种变体：带动量的SGD、Adagrad、RMSProp等等。这些变体通称为优化方法或者优化器（optimizer）。
